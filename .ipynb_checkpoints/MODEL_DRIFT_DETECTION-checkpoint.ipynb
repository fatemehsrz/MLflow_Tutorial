{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca052d93-0842-4b2e-83fe-69edbdf8ba6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installing the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d35ac5b-a00e-4df8-84af-82ec74ca33ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/goli/anaconda3/lib/python3.8/site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in /home/goli/anaconda3/lib/python3.8/site-packages (from xgboost) (1.19.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.5\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/home/goli/anaconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install xgboost    #xgboost==1.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register a model via MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1bb2b85-085b-4e9a-9ca4-e79686944a5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def RRMSE(true, pred):\n",
    "\n",
    "    num = np.sum(np.square(true - pred))\n",
    "    den = np.sum(np.square(pred))\n",
    "    \n",
    "    squared_error = num/den\n",
    "    rrmse_loss = np.sqrt(squared_error)\n",
    "    \n",
    "    return rrmse_loss\n",
    "\n",
    "\n",
    "def train_register_model(df=None, model_name=None):\n",
    "\n",
    "\n",
    "     with mlflow.start_run(run_name=model_name):\n",
    "        \n",
    "    \n",
    "        params={'learning_rate':0.1,\n",
    "            'max_depth':8,\n",
    "            'n_estimators':1000}     \n",
    "      \n",
    "        mlflow.log_params(params) # manual log\n",
    "      \n",
    "        mlflow.autolog() # automatic log\n",
    "\n",
    "        xg_model = XGBRegressor(**params,\n",
    "                                 min_child_weight=0,\n",
    "                                 gamma=0.7,\n",
    "                                 subsample=0.7,\n",
    "                                 colsample_bytree=0.7,\n",
    "                                 objective='reg:squarederror', #'reg:linear',\n",
    "                                 nthread=-1,\n",
    "                                 scale_pos_weight=1,\n",
    "                                 seed=25,\n",
    "                                 reg_alpha=0.00006,\n",
    "                                 random_state=42)\n",
    "        \n",
    "        \n",
    "        X_train ,y_train= prepare_Xy(df)\n",
    "        \n",
    "        xg_model.fit(X_train , y_train)\n",
    "\n",
    "        print('trining finished ...')\n",
    "\n",
    "\n",
    "        mlflow.xgboost.log_model(xg_model, \n",
    "                         artifact_path=\"XGBoost_models/%s\"%model_name, \n",
    "                          registered_model_name=\"%s\"%model_name)\n",
    "        \n",
    "        print('model %s has been registered '%model_name, '\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3449a8a2-7198-47d7-9824-8a0a98ca7867",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_Xy(df):\n",
    "  \n",
    "    y=df.pop('target')\n",
    "\n",
    "    X=df\n",
    "  \n",
    "\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the lastest version of the model a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68d3706-0bb2-4fe8-a649-f595e23ccb99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "from mlflow.entities.model_registry.model_version_status import ModelVersionStatus\n",
    "import mlflow\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def fetch_model_last_update(model_name):\n",
    "\n",
    "\n",
    "    client = MlflowClient() \n",
    "\n",
    "    vf= client.get_latest_versions(model_name, stages=[\"None\"])\n",
    "    \n",
    "    lastest_model= vf[0].version\n",
    "\n",
    "    run_id= vf[0].run_id\n",
    "    run = mlflow.get_run(run_id)\n",
    "    run.data.tags['mlflow.log-model.history'] \n",
    "    \n",
    "    DT_split= DT.split('\":\"')[-3][:10]\n",
    "    REG_DATE=DT_split.split('-')\n",
    "\n",
    "    print('Model:',model_name, '  last training date:', REG_DATE[0]+'-'+REG_DATE[1]+'-'+REG_DATE[2])\n",
    "     \n",
    "    model_last_update= date(int(REG_DATE[0]), int(REG_DATE[1]), int(REG_DATE[2]))\n",
    "\n",
    "    return model_last_update\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c9033f-1ab2-4f69-b434-eee37142f5ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "from datetime import date, datetime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import time, threading\n",
    "\n",
    "\n",
    "\n",
    "def test_prediction( df_test, model_name ):\n",
    "\n",
    "\n",
    "    client = MlflowClient() \n",
    "\n",
    "    vf= client.get_latest_versions(group, stages=[\"None\"])\n",
    "  \n",
    "    model_test = mlflow.pyfunc.load_model(model_uri=\"models:/%s/%s\"% (model_name, vf[0].version)) \n",
    "\n",
    "    \n",
    "    X_test, y_test= prepare_Xy(df_test)\n",
    "                      \n",
    "\n",
    "    y_pred= np.array(model_test.predict(X_test))\n",
    "\n",
    "    rrmse= RRMSE(y_test, y_pred)\n",
    "    r2= r2_score( y_test, y_pred)\n",
    "\n",
    "    return rrmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96bec554-d058-4cca-90a5-15ee0d3b4790",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "\n",
    "def model_exist(model_name):\n",
    "\n",
    "  client = MlflowClient() \n",
    "\n",
    "  try:\n",
    "      print('existence check ...')\n",
    "      client.get_latest_versions(odel_name, stages=[\"None\"])\n",
    "\n",
    "  except:\n",
    "       print(odel_name, \"***** Model does not exist ****** \", '\\n')\n",
    "\n",
    "       return False\n",
    "\n",
    "  return True\n",
    "\n",
    "\n",
    "\n",
    "def update_data():\n",
    "     %run ./Extract_data_and_merging_\n",
    "     %run ./preprocessing_feature_engineering_\n",
    "\n",
    "    #dbutils.notebook.run(\"Extract_data_and_merging_\", 60)\n",
    "    #butils.notebook.run(\"preprocessing_feature_engineering_\", 60)\n",
    "\n",
    "\n",
    "def fetch_new_data():\n",
    "\n",
    "  old_data= spark.read.options(header='True', inferSchema='True', delimiter=',').csv(\"/dbfs/mnt/data\")\n",
    "                                                                                    \n",
    "  print('old data has been loaded ...')\n",
    "\n",
    "  update_data()\n",
    "\n",
    "  updated_data= spark.read.options(header='True', inferSchema='True', delimiter=',').csv(\"/dbfs/mnt/data\")\n",
    "  updated_dataset= updated_data.toPandas()\n",
    "\n",
    "  print('updated data has been loaded  ...')\n",
    "\n",
    "\n",
    "  if not updated_dataset.equals(old_dataset):\n",
    "\n",
    "      new_records= pd.concat([updated_data, old_data]).drop_duplicates(keep=False)\n",
    "\n",
    "      print('new data records ready ...')\n",
    "\n",
    "      return new_records, updated_dataset  \n",
    "\n",
    "  else:\n",
    "      return   old_dataset, updated_dataset  \n",
    "\n",
    "\n",
    "\n",
    "def model_drift_detector( model_list):\n",
    "\n",
    "    new_records, updated_dataset =fetch_new_data(df)\n",
    " \n",
    "\n",
    "    for model_name in model_list:\n",
    "\n",
    "        if model_exist(model_name):\n",
    "\n",
    "            test_rrmse, test_r2= test_prediction(new_records, model_name)\n",
    "\n",
    "            print('Test RRMSE:', test_rrmse, '   R2:', test_r2)\n",
    "\n",
    "\n",
    "            if test_rrmse >0.2 and test_r2 <0.9: \n",
    "\n",
    "              print('Drift detected: model should be retrained...')\n",
    "\n",
    "              train_register_model(updated_dataset, model_name)\n",
    "\n",
    "            else:\n",
    "              print('No drift detected ...', '\\n')\n",
    "\n",
    "        else:\n",
    "            continue \n",
    "      \n",
    "     \n",
    "\n",
    "\n",
    "def monitor_model():\n",
    "\n",
    "    while True:\n",
    "    \n",
    "       model_drift_detector(model_list)\n",
    "       time.sleep( 2629800 ) # monthly check\n",
    "\n",
    "#monitor_model() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c09a6b-95eb-42ba-8f05-ebe42f2ccbf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203ff69b-023e-4085-ae15-c3afe7f5ae8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53bfa39d-dc12-4ef0-a0eb-c98aecfda05f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DRIFT_NEW_DATA",
   "notebookOrigID": 415565459887491,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
